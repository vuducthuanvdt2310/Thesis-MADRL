# üìã PHASE 4: Suggestions & Glossary

## üìñ GLOSSARY OF TECHNICAL TERMS

Understanding the jargon is crucial! Here's your comprehensive dictionary for this codebase.

---

### **A. REINFORCEMENT LEARNING FUNDAMENTALS** üéÆ

#### **Agent**
**Simple:** The "player" or decision-maker in the system  
**Technical:** An entity that observes the environment state and takes actions to maximize cumulative reward  
**In this code:** Each stage in the supply chain (Manufacturer, Distributor, Retailer)

#### **Environment (Env)**
**Simple:** The "game world" or simulation  
**Technical:** The system that responds to agent actions, provides observations, and assigns rewards  
**In this code:** The supply chain simulator in [envs/serial.py](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/envs/serial.py) or [envs/net_2x3.py](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/envs/net_2x3.py)

#### **State / Observation**
**Simple:** What the agent "sees" at any given moment  
**Technical:** The information available to the agent describing the current situation  
**In this code:** Inventory level, backlog, demand, pipeline orders (7 values per agent)

#### **Action**
**Simple:** What the agent "does"  
**Technical:** The decision made by the agent (e.g., how much to order)  
**In this code:** Discrete values from 0-20 representing order quantities

#### **Reward**
**Simple:** Feedback signal (like a video game score)  
**Technical:** Numerical value indicating how good/bad an action was (positive = good, negative = cost)  
**In this code:** Negative values representing costs (holding, backlog, ordering)

#### **Policy (œÄ)**
**Simple:** The agent's strategy or "brain"  
**Technical:** A mapping from observations to actions (what to do in each situation)  
**In this code:** The Actor neural network that outputs action probabilities

#### **Episode**
**Simple:** One complete playthrough of the game  
**Technical:** A sequence of interactions from initial state to terminal state  
**In this code:** 200 time steps of supply chain operation

#### **Trajectory / Rollout**
**Simple:** The path taken during an episode  
**Technical:** Sequence of (state, action, reward, next_state) tuples  
**In this code:** 200 steps √ó 5 parallel threads = 1000 experiences per episode

---

### **B. MULTI-AGENT CONCEPTS** üë•

#### **Multi-Agent Reinforcement Learning (MARL)**
**Simple:** Multiple AI agents learning together  
**Technical:** Multiple autonomous agents learning simultaneously in a shared environment  
**In this code:** 3 agents (serial) or 6 agents (network) learning coordinated inventory policies

#### **Centralized Training, Decentralized Execution (CTDE)**
**Simple:** Learn together, act independently  
**Technical:** During training, agents share information; during execution, each acts on local observations  
**In this code:** Critic uses centralized observations (sees all agents), Actor uses local observations

#### **Heterogeneous Agents**
**Simple:** Agents with different capabilities  
**Technical:** Agents that may have different observation/action spaces or roles  
**In this code:** Each supply chain stage has different characteristics (retailers vs manufacturers)

#### **Cooperative MARL**
**Simple:** Agents work together toward a common goal  
**Technical:** All agents aim to maximize a shared or aligned objective  
**In this code:** All agents minimize total supply chain cost

---

### **C. ALGORITHM-SPECIFIC TERMS** üßÆ

#### **HAPPO (Heterogeneous-Agent Proximal Policy Optimization)**
**Simple:** The specific AI learning algorithm used  
**Technical:** A multi-agent extension of PPO that handles heterogeneous agents with sequential policy updates  
**Key Feature:** Ensures monotonic improvement and stable learning

#### **PPO (Proximal Policy Optimization)**
**Simple:** A safe way to update the AI's strategy  
**Technical:** An on-policy RL algorithm that limits how much the policy can change in each update  
**Key Mechanism:** Clipping ratio to [0.8, 1.2] prevents drastic policy changes

#### **Actor-Critic**
**Simple:** Two-part AI system: decision maker + quality judge  
**Technical:** Architecture combining a policy network (actor) and value function (critic)  
- **Actor:** Learns what actions to take ‚Üí [algorithms/actor_critic.py](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/algorithms/actor_critic.py) - Actor class
- **Critic:** Learns to predict future returns ‚Üí [algorithms/actor_critic.py](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/algorithms/actor_critic.py) - Critic class

#### **On-Policy vs Off-Policy**
**Simple:** Learning from current behavior vs past behavior  
**On-Policy:** Learn only from data generated by current policy (HAPPO/PPO is on-policy)  
**Off-Policy:** Can learn from old experiences (DQN, SAC are off-policy)

#### **Trust Region**
**Simple:** A "safety zone" for policy updates  
**Technical:** Constraint ensuring new policy stays close to old policy  
**In this code:** Implemented via PPO clipping in [happo_trainer.py](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/algorithms/happo_trainer.py)

---

### **D. NEURAL NETWORK TERMS** üß†

#### **Neural Network**
**Simple:** The AI's "brain" made of interconnected neurons  
**Technical:** Computational model with layers of nodes that learn to map inputs to outputs  
**In this code:** 2 hidden layers with 128 neurons each

#### **Forward Pass**
**Simple:** Running data through the network  
**Technical:** Computing output by passing input through layers sequentially  
**In this code:** `actor.forward()` takes observation, outputs action

#### **Backward Pass / Backpropagation**
**Simple:** How the network learns from mistakes  
**Technical:** Computing gradients and updating weights using chain rule  
**In this code:** Done automatically by PyTorch `optimizer.step()`

#### **Optimizer**
**Simple:** The learning mechanism  
**Technical:** Algorithm that updates network weights to minimize loss  
**In this code:** Adam optimizer with learning rate 1e-4

#### **Learning Rate (lr)**
**Simple:** How fast the AI learns  
**Technical:** Step size for weight updates during gradient descent  
**In this code:** 0.0001 for both actor and critic

#### **Activation Function**
**Simple:** Adds non-linearity to the network  
**Technical:** Function applied to neuron outputs (e.g., ReLU, Tanh)  
**In this code:** ReLU for hidden layers, Tanh for outputs

#### **RNN (Recurrent Neural Network)**
**Simple:** Memory-enabled neural network  
**Technical:** Network with loops that can maintain state across time steps  
**In this code:** Optional feature (`use_naive_recurrent_policy=True`)

---

### **E. TRAINING CONCEPTS** üèãÔ∏è

#### **Advantage**
**Simple:** How much better an action was than expected  
**Technical:** A(s,a) = Q(s,a) - V(s) = "actual value - baseline value"  
**Formula:** `advantage = return - value_prediction`  
**Purpose:** Reduces variance in policy gradient updates

#### **GAE (Generalized Advantage Estimation)**
**Simple:** A smoother way to calculate advantages  
**Technical:** Exponentially-weighted average of n-step advantages  
**In this code:** Used in [separated_buffer.py](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/utils/separated_buffer.py) ‚Üí [compute_returns()](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/utils/separated_buffer.py#124-170)  
**Parameters:** Œ≥ (gamma) = 0.95, Œª (gae_lambda) = 0.95

#### **Return**
**Simple:** Total reward accumulated from a state  
**Technical:** Discounted sum of future rewards: R_t = Œ£ Œ≥^k * r_{t+k}  
**In this code:** Computed backwards from episode end

#### **Discount Factor (Œ≥, gamma)**
**Simple:** How much to value future rewards  
**Technical:** Value between 0-1 that reduces importance of distant rewards  
**In this code:** Œ≥ = 0.95 (future is 95% as valuable as present)

#### **Entropy**
**Simple:** Measure of randomness in decisions  
**Technical:** Quantifies uncertainty in action distribution  
**Purpose:** Encourages exploration (prevents premature convergence)  
**In this code:** `entropy_coef = 0.01`

#### **Clip Parameter**
**Simple:** Safety limit on policy changes  
**Technical:** PPO clipping range (typically 0.2)  
**In this code:** `clip_param = 0.2` ‚Üí ratio clipped to [0.8, 1.2]

#### **Gradient Clipping**
**Simple:** Prevents learning steps that are too large  
**Technical:** Caps gradient norm to prevent exploding gradients  
**In this code:** `max_grad_norm = 0.5` (when enabled)

---

### **F. VALUE NORMALIZATION** üìä

#### **PopArt (Preserve Outputs Precisely, Affine Re-parametrization of Targets)**
**Simple:** Keeps reward scales manageable  
**Technical:** Normalizes value function outputs while preserving learned representations  
**In this code:** `utils/popart.py` (optional feature)

#### **Value Normalization**
**Simple:** Standardizes value predictions  
**Technical:** Running normalization of value targets (mean=0, std=1)  
**In this code:** `utils/valuenorm.py` ‚Üí `use_valuenorm=True`

---

### **G. SUPPLY CHAIN SPECIFIC** üì¶

#### **Multi-Echelon Inventory**
**Simple:** Managing stock at multiple levels  
**Technical:** Inventory management across different stages of a supply chain  
**In this code:** 3-stage (serial) or 6-stage (network) systems

#### **Echelon**
**Simple:** A level or stage in the supply chain  
**Technical:** A node in the distribution network (factory, warehouse, retailer)  
**In this code:** Each agent controls one echelon

#### **Backlog / Backorder**
**Simple:** Unfulfilled customer orders  
**Technical:** Demand that couldn't be met due to stockout (negative inventory)  
**Cost:** High penalty to encourage keeping stock

#### **Pipeline Inventory**
**Simple:** Orders in transit  
**Technical:** Items ordered but not yet received (lead time)  
**In this code:** Tracked in state variable `P`

#### **Lead Time**
**Simple:** Delay between ordering and receiving  
**Technical:** Time lag for orders to arrive  
**In this code:** 2 time periods for most environments

#### **Holding Cost**
**Simple:** Cost of storing inventory  
**Technical:** h * max(0, inventory) per period  
**In this code:** Varies by echelon (typical h=1)

#### **Stockout Cost / Backlog Cost**
**Simple:** Penalty for not having enough stock  
**Technical:** b * max(0, -inventory) per period  
**In this code:** Usually higher than holding cost (typical b=4)

#### **Bullwhip Effect**
**Simple:** Demand variability amplification upstream  
**Technical:** Phenomenon where order variance increases moving from customer to supplier  
**In this code:** Measured as "ordering fluctuation" in evaluation

---

### **H. IMPLEMENTATION DETAILS** üíª

#### **Replay Buffer / Experience Buffer**
**Simple:** Memory storage for experiences  
**Technical:** Data structure storing (s, a, r, s') tuples for training  
**In this code:** [utils/separated_buffer.py](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/utils/separated_buffer.py) ‚Üí [SeparatedReplayBuffer](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/utils/separated_buffer.py#12-427)

#### **Mini-batch**
**Simple:** Small subset of data for training  
**Technical:** Random sample from buffer used for one gradient update  
**In this code:** `num_mini_batch = 1` (uses full episode)

#### **Epoch**
**Simple:** One full pass through the training data  
**Technical:** One complete iteration over the replay buffer  
**In this code:** `ppo_epoch = 15` (update 15 times per episode)

#### **Vectorized Environment**
**Simple:** Running multiple environments in parallel  
**Technical:** Batch processing multiple environment instances simultaneously  
**In this code:** `SubprocVecEnv` runs 5 parallel environments

#### **Seed**
**Simple:** Starting point for randomness  
**Technical:** Initial value for random number generator (ensures reproducibility)  
**In this code:** `seed = [0,1,2,3,4,5,6,7,8,9]` for 10 runs

#### **Deterministic Policy**
**Simple:** Always choosing the best action (no randomness)  
**Technical:** Selecting argmax(policy) instead of sampling  
**In this code:** Used during evaluation (`deterministic=True`)

#### **Masks**
**Simple:** Indicators for episode boundaries  
**Technical:** Binary values (0/1) marking episode termination  
**Purpose:** Prevents bootstrapping across episode boundaries

---

### **I. COST/REWARD COMPONENTS** üí∞

#### **Immediate Reward**
**Simple:** Cost incurred in one time step  
**Technical:** r_t = -(holding_cost + backlog_cost + ordering_cost)  
**In this code:** Calculated in [state_update()](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/envs/serial.py#276-340) method

#### **Cumulative Return**
**Simple:** Total cost over entire episode  
**Technical:** Sum of all rewards (discounted): G = Œ£ Œ≥^t * r_t  
**In this code:** What the agent tries to maximize (minimize cost)

#### **Value Function V(s)**
**Simple:** Predicted future cost from a state  
**Technical:** Expected return starting from state s  
**In this code:** Output of Critic network

#### **Action-Value Function Q(s,a)**
**Simple:** Predicted cost of taking action a in state s  
**Technical:** Expected return after taking action a in state s  
**Relation:** A(s,a) = Q(s,a) - V(s)

---

## üîç CODE STATE ANALYSIS

Now let me tell you what I found about this codebase - the good, the potential issues, and what's missing.

---

### ‚úÖ **STRENGTHS OF THIS CODEBASE**

#### **1. Well-Structured Architecture**
- Clear separation of concerns (algorithms, envs, runners, utils)
- Modular design makes it easy to swap components
- Template provided for customization

#### **2. Production-Ready Features**
- **Multi-threading:** Parallel environment execution for speed
- **Checkpointing:** Saves best models automatically
- **Early stopping:** Prevents wasting compute on plateau
- **TensorBoard logging:** Track training progress
- **Evaluation system:** Regular testing on held-out data

#### **3. Comprehensive Implementation**
- Actor-Critic architecture properly implemented
- GAE correctly computed
- PPO clipping mechanism in place
- Value normalization for stability
- Multiple environment topologies (serial, network)

#### **4. Research Quality**
- Based on published paper
- Implements state-of-the-art HAPPO algorithm
- Includes proper baselines and evaluation metrics

---

### ‚ö†Ô∏è **POTENTIAL ISSUES & LIMITATIONS**

#### **1. Limited Documentation**
**Issue:** Minimal inline comments explaining complex logic  
**Impact:** Hard for beginners to understand what's happening  
**Example:** [state_update()](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/envs/serial.py#276-340) method lacks step-by-step explanation  

**Suggestion:**
```python
# Add detailed docstrings like:
def state_update(self, action):
    """
    Update supply chain state based on actions.
    
    Process flow:
    1. Receive shipments from upstream suppliers
    2. Satisfy backlog with available inventory
    3. Fulfill new customer demand
    4. Update inventory levels
    5. Calculate period costs
    
    Args:
        action: List of order quantities for each echelon
    
    Returns:
        rewards: List of costs (negative rewards) for each agent
    """
```

#### **2. Hardcoded Parameters**
**Issue:** Many parameters embedded in environment files  
**Impact:** Difficult to run experiments with different settings  
**Example:** In [serial.py](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/envs/serial.py):
```python
h = [1,1,1]  # Holding costs - hardcoded
b = [4,4,4]  # Backlog costs - hardcoded
```

**Suggestion:** Move to config file:
```python
parser.add_argument("--holding_costs", type=list, default=[1,1,1])
parser.add_argument("--backlog_costs", type=list, default=[4,4,4])
```

#### **3. No Hyperparameter Tuning Framework**
**Issue:** No grid search or automated tuning  
**Impact:** Finding optimal hyperparameters requires manual trial-and-error  

**Suggestion:** Integrate with tools like:
- Optuna for automated hyperparameter search
- Ray Tune for distributed optimization

#### **4. Limited Environment Diversity**
**Issue:** Only 2 pre-built environments (serial, network)  
**Impact:** May not generalize to other supply chain structures  

**Suggestion:** Add more variants:
- Different network topologies (star, tree, mesh)
- Varying lead times
- Multiple product types
- Capacity constraints

#### **5. No Pre-trained Models**
**Issue:** No example trained models provided  
**Impact:** Users must train from scratch (takes hours)  

**Suggestion:** Include:
- Pre-trained checkpoints for quick testing
- Model zoo with different configurations

#### **6. Evaluation Metrics**
**Issue:** Limited metrics (just average reward and bullwhip)  
**Impact:** Hard to deeply analyze agent behavior  

**Suggestion:** Add metrics:
- Service level (% of demand met)
- Inventory turnover ratio
- Order frequency distribution
- Per-echelon cost breakdown

---

### üöß **MISSING COMPONENTS**

#### **1. Visualization Tools**
**Missing:**
- Plots of inventory over time
- Action distributions
- Learning curves
- State transition diagrams

**Would Help:**
- Debug training issues
- Understand agent strategies
- Present results in papers

**Quick Win:** Add plotting scripts using matplotlib:
```python
# plot_results.py
import matplotlib.pyplot as plt
import numpy as np

def plot_inventory_trajectory(inventory_log):
    """Plot inventory levels over episode"""
    plt.figure(figsize=(12,6))
    for i, inv in enumerate(inventory_log):
        plt.plot(inv, label=f'Echelon {i+1}')
    plt.xlabel('Time Step')
    plt.ylabel('Inventory Level')
    plt.legend()
    plt.savefig('inventory_trajectory.png')
```

#### **2. Baseline Comparisons**
**Missing:**
- Classic inventory policies (Base-Stock, (s,S), Economic Order Quantity)
- Independent learning (IPPO)
- Other MARL algorithms (MADDPG, QMIX)

**Would Help:**
- Demonstrate HAPPO's superiority
- Validate results
- Academic rigor

#### **3. Unit Tests**
**Missing:**
- Test coverage for critical functions
- Integration tests
- Regression tests

**Risk:**
- Silent bugs in complex logic
- Breaking changes go unnoticed

**Suggestion:** Add pytest tests:
```python
# tests/test_environment.py
def test_inventory_update():
    env = SerialEnv()
    initial_inventory = env.I.copy()
    action = [10, 10, 10]
    env.step(action)
    # Assert inventory changed correctly
    assert env.I[0] <= initial_inventory[0] + 10
```

#### **4. Deployment Scripts**
**Missing:**
- Script to load trained model for inference
- API or web interface
- Real-time decision support

**Would Help:**
- Use trained policies in practice
- Demonstrate real-world applicability

#### **5. Experiment Tracking**
**Missing:**
- Integration with MLflow, Weights & Biases
- Experiment versioning
- Automatic hyperparameter logging

**Would Help:**
- Reproduce results
- Compare across runs
- Share with collaborators

---

### üéØ **SUGGESTIONS FOR IMPROVEMENT**

#### **Priority 1: Documentation** üìö
1. Add comprehensive README with:
   - Installation troubleshooting
   - Expected training time
   - Performance benchmarks
   - FAQ section
   
2. Create tutorial notebook:
   - Step-by-step walkthrough
   - Visualizations
   - Explanation of outputs
   
3. Add architecture diagram showing data flow

#### **Priority 2: Usability** üé®
1. Create configuration presets:
   ```python
   # configs/quick_test.py
   num_env_steps = 10000  # Fast testing
   
   # configs/full_training.py
   num_env_steps = 3000000  # Production
   ```

2. Add progress bars (using tqdm):
   ```python
   from tqdm import tqdm
   for episode in tqdm(range(episodes), desc="Training"):
       # training code
   ```

3. Implement automatic result visualization

#### **Priority 3: Robustness** üõ°Ô∏è
1. Add input validation:
   ```python
   def step(self, actions):
       assert len(actions) == self.num_agents, \
           f"Expected {self.num_agents} actions, got {len(actions)}"
       # rest of code
   ```

2. Exception handling:
   ```python
   try:
       model = torch.load(model_path)
   except FileNotFoundError:
       print(f"Model not found at {model_path}")
       # fallback behavior
   ```

3. Add logging instead of prints:
   ```python
   import logging
   logger = logging.getLogger(__name__)
   logger.info(f"Training episode {episode}/{total_episodes}")
   ```

#### **Priority 4: Extensibility** üîß
1. Create plugin system for custom reward functions
2. Allow custom demand generators
3. Support for continuous action spaces

---

### üìä **WHAT'S ACTUALLY MISSING (Critical)**

After thorough review, here's what would make this code truly complete:

#### **1. Installation Dependencies**
**Current:** [requirements.txt](file:///d:/thuan/thesis/Multi-Agent-Deep-Reinforcement-Learning-on-Multi-Echelon-Inventory-Management/requirements.txt) exists ‚úì  
**Missing:** Specific Python version requirement in README  
**Risk:** Compatibility issues with Python 3.9+

#### **2. Data Files**
**Current:** Test data folder exists ‚úì  
**Missing:** Clear explanation of data format  
**Risk:** Users can't create custom demand patterns easily

#### **3. Model Checkpoint Format**
**Current:** Models saved with torch.save ‚úì  
**Missing:** Metadata (hyperparameters, training stats)  
**Risk:** Can't reproduce results from checkpoint alone

#### **4. Results Directory**
**Current:** Auto-created outside project ‚úì  
**Issue:** Path is `../results` which can be confusing  
**Suggestion:** Make path configurable

---

## üéì **FOR YOUR THESIS - WHAT TO FOCUS ON**

Since you're learning this for your thesis, here's what's most important:

### **Must Understand Deeply:**
1. ‚úÖ **HAPPO algorithm** - The core contribution
2. ‚úÖ **Actor-Critic architecture** - How decisions are made
3. ‚úÖ **Multi-agent coordination** - Why MARL is needed
4. ‚úÖ **Supply chain dynamics** - Domain knowledge

### **Should Understand:**
1. GAE and advantage estimation
2. PPO clipping mechanism
3. Centralized training, decentralized execution
4. Replay buffer mechanics

### **Nice to Understand:**
1. Value normalization details
2. RNN for temporal dependencies
3. Parallel environment implementation
4. TensorBoard logging

### **Can Treat as Black Box:**
1. PyTorch internals (autograd)
2. Subprocess management
3. Numpy optimization tricks

---

## üî¨ **EXPERIMENTAL SUGGESTIONS**

If you want to extend this for your thesis:

### **Ablation Studies:**
1. Remove centralized critic ‚Üí compare performance
2. Disable value normalization ‚Üí measure instability
3. Vary number of agents ‚Üí scalability analysis
4. Different network topologies ‚Üí generalization

### **Novel Extensions:**
1. **Uncertainty modeling:** Add stochastic suppliers (can fail to deliver)
2. **Partial observability:** Agents don't see full state
3. **Communication channels:** Let agents exchange messages
4. **Transfer learning:** Pre-train on simple environment, fine-tune on complex

### **Real-World Features:**
1. Perishable inventory (items expire)
2. Capacity constraints (warehouse limits)
3. Multi-product scenarios
4. Dynamic pricing

---

## ‚úÖ **FINAL VERDICT: CODE QUALITY**

### **Overall Rating: 8/10** üåü

**Pros:**
- ‚úÖ Solid implementation of HAPPO
- ‚úÖ Clean architecture
- ‚úÖ Research-grade quality
- ‚úÖ Extensible design
- ‚úÖ Working parallel execution

**Cons:**
- ‚ùå Limited documentation
- ‚ùå No visualization tools
- ‚ùå Missing baselines
- ‚ùå No unit tests
- ‚ùå Hardcoded parameters

### **Recommendation:**
This is **publication-quality research code**, perfect for:
- ‚úÖ Academic research
- ‚úÖ Learning MARL concepts
- ‚úÖ Baseline for comparisons
- ‚úÖ Thesis work

But needs work for:
- ‚ùå Production deployment
- ‚ùå Non-expert users
- ‚ùå Commercial applications

---

## üéØ **QUICK START CHECKLIST**

Before you run this code, make sure:

- [ ] Python 3.8 installed
- [ ] GPU available (CUDA compatible) - optional but recommended
- [ ] All dependencies installed: `pip install -r requirements.txt`
- [ ] Enough disk space (~1GB for results)
- [ ] Patience (training takes 2-4 hours on GPU, 10+ hours on CPU)

### **Common Issues & Solutions:**

1. **ImportError: No module named 'torch'**
   - Solution: `pip install torch==1.13.0`

2. **CUDA out of memory**
   - Solution: Reduce `n_rollout_threads` from 5 to 2

3. **Training very slow**
   - Solution: Enable GPU by setting `--cuda True`

4. **Results not saving**
   - Solution: Check parent directory write permissions

---

**That's everything for Phase 4!** üéâ

You now have:
- ‚úÖ Complete glossary of all technical terms
- ‚úÖ Code state analysis
- ‚úÖ Strengths and weaknesses
- ‚úÖ Suggestions for improvement
- ‚úÖ Thesis-specific guidance

Any questions about specific terms or concepts? I'm here to help! üöÄ
